{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8386bc36-3e4d-4368-84ae-553330373525",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "\n",
    "This scraping code is built upoin the `scrape.py` code provided\n",
    "\n",
    "All scrapes are in with domain's [robot.txt](https://www.domain.com.au/robots.txt)\n",
    "\n",
    "The scrapping take a long time and is dependent on the date scrape. Because of this a pre-scrapped file is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbd68c-cbd6-4a31-98cb-da48ff800923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from IPython.display import display\n",
    "import time\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "\n",
    "BASE_URL_SUB_PROFILE = \"https://www.domain.com.au/suburb-profile/\"\n",
    "\n",
    "DIR_RAW = \"../data/raw/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "\n",
    "PERCENTAGES = range(1, 101, 5)\n",
    "\n",
    "REQUEST_WAIT_TIME = 0.2\n",
    "\n",
    "# get list of VIC postcodes\n",
    "df_postcodes = pd.read_csv(f\"{DIR_RAW}postcodes.csv\")\n",
    "df_postcodes = df_postcodes[df_postcodes[\"state\"] == \"VIC\"]\n",
    "postcodes = set(df_postcodes[\"postcode\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78431df0-e2c7-4007-b6ba-4f9d09ea7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_display_percentages(xs):\n",
    "    display_percentages = [round(len(xs) * (percentage/100))\n",
    "                           for percentage in PERCENTAGES]\n",
    "    display_percentages[0] = 1\n",
    "    display_percentages[-1] = len(xs)\n",
    "    return display_percentages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8c947-ca8a-4c07-95e2-cd05a1fffd94",
   "metadata": {},
   "source": [
    "## Collect property URLs by postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6e845-6b7e-4d17-9ba4-b5f4fb7145c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_urls = set()\n",
    "\n",
    "postcode_num = 0\n",
    "display_percentage = get_display_percentages(postcodes)\n",
    "\n",
    "for postcode in postcodes:\n",
    "\n",
    "    page_num = 1\n",
    "    while True:\n",
    "\n",
    "        page_url = f\"{BASE_URL}/rent/?postcode={postcode}&page={page_num}\"\n",
    "        content = BeautifulSoup(requests.get(\n",
    "            page_url, headers=HEADERS).text, \"html.parser\")\n",
    "\n",
    "        # collect all candidate property links on page\n",
    "        links = content.find(\"ul\", {\"data-testid\": \"results\"})\n",
    "        if not links:\n",
    "            break\n",
    "\n",
    "        links = links.findAll(\"a\", href=re.compile(f\"{BASE_URL}/*\"))\n",
    "        if not links:\n",
    "            break\n",
    "\n",
    "        for link in links:\n",
    "            if \"address\" in link[\"class\"]:\n",
    "                property_urls.add(link[\"href\"])\n",
    "\n",
    "        page_num += 1\n",
    "\n",
    "        # wait a little time in-between each request to prevent DDOS attack\n",
    "        sleep(REQUEST_WAIT_TIME)\n",
    "\n",
    "    postcode_num += 1\n",
    "    if postcode_num in display_percentage:\n",
    "        print(\n",
    "            f\"{round(postcode_num / len(postcodes) * 100)}% of VIC postcode URLs scraped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c97b0-4f5c-4948-88a7-8ddea4e2cf3b",
   "metadata": {},
   "source": [
    "## Scrape each URL collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fdebde-717c-4751-a69d-3a97805f8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "property_url_num = 0\n",
    "display_percentage = get_display_percentages(property_urls)\n",
    "\n",
    "for property_url in property_urls:\n",
    "\n",
    "    current_scrape = {\"url\": property_url}\n",
    "\n",
    "    content = BeautifulSoup(requests.get(\n",
    "        property_url, headers=HEADERS).text, \"html.parser\")\n",
    "\n",
    "    if not content:\n",
    "        pass\n",
    "\n",
    "    content_price = content.find(\n",
    "        \"div\", {\"data-testid\": \"listing-details__summary-title\"})\n",
    "\n",
    "    content_address = content.find(\"h1\", {\"class\": \"css-164r41r\"})\n",
    "\n",
    "    content_features = content.find(\n",
    "        \"div\", {\"data-testid\": \"property-features\"})\n",
    "    if content_features:\n",
    "        content_features = content_features.findAll(\n",
    "            \"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "\n",
    "    content_property_type = content.find(\n",
    "        \"div\", {\"data-testid\": \"listing-summary-property-type\"})\n",
    "\n",
    "    content_agent = content.find(\n",
    "        \"a\", {\"data-testid\": \"listing-details__agent-details-agent-company-name\"})\n",
    "\n",
    "    content_summary = content.find(\n",
    "        \"div\", {\"data-testid\": \"strip-content-list\"})\n",
    "    if content_summary:\n",
    "        content_summary = content_summary.findAll(\"li\")\n",
    "\n",
    "    content_domain_says = content.find(\n",
    "        \"p\", {\"data-testid\": \"listing-details__domain-says-text\"})\n",
    "\n",
    "    content_neighbourhood_insights = content.find(\n",
    "        \"section\", {\"data-testid\": \"neighbourhood-insights\"})\n",
    "    if content_neighbourhood_insights:\n",
    "        content_age = content.findAll(\n",
    "            \"tr\", {\"data-testid\": \"neighbourhood-insights__age-brackets-row\"})\n",
    "        content_long_term_residents = content.find(\n",
    "            \"div\", {\"data-testid\": \"single-value-doughnut-graph\"})\n",
    "        content_type = content.findAll(\"div\", {\"class\": \"css-14hea9r\"})\n",
    "    else:\n",
    "        content_age = content_long_term_residents = content_type = None\n",
    "\n",
    "    content_stats = content.find(\n",
    "        \"div\", {\"data-testid\": \"listing-details__suburb-insights\"})\n",
    "    if content_stats:\n",
    "        content_values = content_stats.findAll(\"div\", {\"class\": \"css-35ezg3\"})\n",
    "\n",
    "        content_occupancy = content_stats.find(\n",
    "            \"div\", {\"data-testid\": \"suburb-insights__occupancy\"})\n",
    "        content_household = content_stats.find(\n",
    "            \"div\", {\"data-testid\": \"suburb-insights__household\"})\n",
    "    else:\n",
    "        content_values = content_occupancy = content_household = None\n",
    "\n",
    "    content_coordinates = content.find(\n",
    "        \"a\", {\"target\": \"_blank\", \"rel\": \"noopener noreferer\"})\n",
    "\n",
    "    if content_price:\n",
    "        current_scrape[\"price\"] = content_price.getText()\n",
    "\n",
    "    if content_address:\n",
    "        current_scrape[\"address\"] = content_address.getText()\n",
    "\n",
    "    if content_features and len(content_features) >= 1:\n",
    "        current_scrape[\"num_beds\"] = content_features[0].getText()\n",
    "\n",
    "    if content_features and len(content_features) >= 2:\n",
    "        current_scrape[\"num_bath\"] = content_features[1].getText()\n",
    "\n",
    "    if content_features and len(content_features) >= 3:\n",
    "        current_scrape[\"num_car\"] = content_features[2].getText()\n",
    "\n",
    "    if content_property_type:\n",
    "        current_scrape[\"property_type\"] = content_property_type.getText()\n",
    "\n",
    "    if content_agent:\n",
    "        current_scrape[\"agent\"] = content_agent.getText()\n",
    "\n",
    "    if content_summary:\n",
    "        for entry in content_summary:\n",
    "            bond_found = re.findall(r\"([bB]ond \\$[0-9,\\.]+)\",  entry.getText())\n",
    "            internal_area_found = re.findall(\n",
    "                r\"([iI]nternal area .+)\",  entry.getText())\n",
    "            land_area_found = re.findall(\n",
    "                r\"([lL]and area .+)\",  entry.getText())\n",
    "\n",
    "            if bond_found:\n",
    "                current_scrape[\"bond\"] = bond_found[0]\n",
    "\n",
    "            if internal_area_found:\n",
    "                current_scrape[\"internal_area\"] = internal_area_found[0]\n",
    "\n",
    "            if land_area_found:\n",
    "                current_scrape[\"land_area\"] = land_area_found[0]\n",
    "\n",
    "    if content_domain_says:\n",
    "        current_scrape[\"domain_says\"] = content_domain_says.getText()\n",
    "\n",
    "    if content_age and len(content_age) >= 4:\n",
    "        content_under_20 = content_age[0].find(\n",
    "            \"div\", {\"data-testid\": \"bar-value\"})\n",
    "        content_20_to_39 = content_age[1].find(\n",
    "            \"div\", {\"data-testid\": \"bar-value\"})\n",
    "        content_40_to_59 = content_age[2].find(\n",
    "            \"div\", {\"data-testid\": \"bar-value\"})\n",
    "        content_above_60 = content_age[3].find(\n",
    "            \"div\", {\"data-testid\": \"bar-value\"})\n",
    "\n",
    "        if content_under_20:\n",
    "            current_scrape[\"neighbourhood_under_20\"] = content_under_20.getText()\n",
    "\n",
    "        if content_20_to_39:\n",
    "            current_scrape[\"neighbourhood_20_to_39\"] = content_20_to_39.getText()\n",
    "\n",
    "        if content_40_to_59:\n",
    "            current_scrape[\"neighbourhood_40_to_59\"] = content_40_to_59.getText()\n",
    "\n",
    "        if content_above_60:\n",
    "            current_scrape[\"neighbourhood_above_60\"] = content_above_60.getText()\n",
    "\n",
    "    if content_long_term_residents:\n",
    "        current_scrape[\"neighbourhood_long_term_residents\"] = content_long_term_residents.getText()\n",
    "\n",
    "    if content_type and len(content_type) >= 4:\n",
    "        content_owners = content_type[0].find(\n",
    "            \"span\", {\"data-testid\": \"left-value\"})\n",
    "        content_renter = content_type[0].find(\n",
    "            \"span\", {\"data-testid\": \"right-value\"})\n",
    "        content_family = content_type[1].find(\n",
    "            \"span\", {\"data-testid\": \"left-value\"})\n",
    "        content_single = content_type[1].find(\n",
    "            \"span\", {\"data-testid\": \"right-value\"})\n",
    "\n",
    "        if content_owners:\n",
    "            current_scrape[\"neighbourhood_owners\"] = content_owners.getText()\n",
    "\n",
    "        if content_renter:\n",
    "            current_scrape[\"neighbourhood_renter\"] = content_renter.getText()\n",
    "\n",
    "        if content_family:\n",
    "            current_scrape[\"neighbourhood_family\"] = content_family.getText()\n",
    "\n",
    "        if content_single:\n",
    "            current_scrape[\"neighbourhood_single\"] = content_single.getText()\n",
    "\n",
    "    if content_values and len(content_values) >= 6:\n",
    "        current_scrape[\"performance_median_price\"] = content_values[0].getText()\n",
    "        current_scrape[\"performance_auction_clearance\"] = content_values[1].getText()\n",
    "        current_scrape[\"performance_sold_this_year\"] = content_values[2].getText()\n",
    "        current_scrape[\"performance_avg_days_on_market\"] = content_values[3].getText()\n",
    "\n",
    "        current_scrape[\"demographic_population\"] = content_values[4].getText()\n",
    "        current_scrape[\"demographic_average_age\"] = content_values[5].getText()\n",
    "\n",
    "    if content_occupancy:\n",
    "        content_owners = content_occupancy.find(\n",
    "            \"span\", {\"data-testid\": \"left-value\"})\n",
    "        content_renter = content_occupancy.find(\n",
    "            \"span\", {\"data-testid\": \"right-value\"})\n",
    "\n",
    "        if content_owners:\n",
    "            current_scrape[\"demographic_owner\"] = content_owners.getText()\n",
    "\n",
    "        if content_renter:\n",
    "            current_scrape[\"demographic_renter\"] = content_renter.getText()\n",
    "\n",
    "    if content_household:\n",
    "        content_family = content_household.find(\n",
    "            \"span\", {\"data-testid\": \"left-value\"})\n",
    "        content_single = content_household.find(\n",
    "            \"span\", {\"data-testid\": \"right-value\"})\n",
    "\n",
    "        if content_family:\n",
    "            current_scrape[\"demographic_family\"] = content_family.getText()\n",
    "\n",
    "        if content_single:\n",
    "            current_scrape[\"demographic_single\"] = content_single.getText()\n",
    "\n",
    "    if content_coordinates:\n",
    "        coordinates = re.findall(\n",
    "            r\"destination=([-\\s,\\d\\.]+)\", content_coordinates.attrs[\"href\"])\n",
    "        if coordinates:\n",
    "            coordinates = coordinates[0].split(\",\")\n",
    "            current_scrape[\"latitude\"] = coordinates[0]\n",
    "            current_scrape[\"longitude\"] = coordinates[1]\n",
    "\n",
    "    data.append(current_scrape)\n",
    "\n",
    "    # wait a little time in-between each request to prevent DDOS attack\n",
    "    sleep(REQUEST_WAIT_TIME)\n",
    "\n",
    "    property_url_num += 1\n",
    "    if property_url_num in display_percentage:\n",
    "        print(\n",
    "            f\"{round(property_url_num / len(property_urls) * 100)}% of property URLs scraped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371240f-50bd-458d-99a8-348dbd64eb19",
   "metadata": {},
   "source": [
    "## Save scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48ae46-3525-4e58-a39a-38c0c74c43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrape = pd.json_normalize(data)\n",
    "display(df_scrape.head(10))\n",
    "\n",
    "df_scrape.to_csv(f\"{DIR_RAW}/scraped_properties.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5368e14",
   "metadata": {},
   "source": [
    "# Historical Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20935e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_setup():\n",
    "    \"\"\"Set up driver configs\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--ignore-certificate-errors\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--headless\")\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path=ChromeDriverManager().install(), chrome_options=options\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "\n",
    "def df_setup():\n",
    "    \"\"\"Setup dataframe\"\"\"\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"suburb\",\n",
    "            \"postcode\",\n",
    "            \"p_type\",\n",
    "            \"median_sell\",\n",
    "            \"avg_days_on_market\",\n",
    "            \"clearance\",\n",
    "            \"n_sold\",\n",
    "            \"h_range\",\n",
    "            \"l_range\",\n",
    "            \"median_rent\",\n",
    "            \"2022_median\",\n",
    "            \"2022_growth\",\n",
    "            \"2022_n_sold\",\n",
    "            \"2021_median\",\n",
    "            \"2021_growth\",\n",
    "            \"2021_n_sold\",\n",
    "            \"2020_median\",\n",
    "            \"2020_growth\",\n",
    "            \"2020_n_sold\",\n",
    "            \"2019_median\",\n",
    "            \"2019_growth\",\n",
    "            \"2019_n_sold\",\n",
    "            \"2018_median\",\n",
    "            \"2018_growth\",\n",
    "            \"2018_n_sold\",\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def url_finder(row):\n",
    "    suburb = row[\"locality\"].lower().replace(r\" \", \"-\")\n",
    "    postcode = row[\"postcode\"]\n",
    "\n",
    "    url = f\"{BASE_URL_SUB_PROFILE}{suburb}-vic-{postcode}\"\n",
    "\n",
    "    return url, suburb\n",
    "\n",
    "\n",
    "def interactive_data(soup, index):\n",
    "    \"\"\"Find new data from interactive element\"\"\"\n",
    "    all = []\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        cols = [td.text for td in tr.find_all(\"td\")]\n",
    "        all.append(cols)\n",
    "    data = all[index][0]\n",
    "    data = data.replace(\"\\xa0\", \" \")\n",
    "    return data\n",
    "\n",
    "\n",
    "def yearly_history_parser(regex):\n",
    "    try:\n",
    "        median = regex.group(1)\n",
    "    except AttributeError as e:\n",
    "        median = None\n",
    "\n",
    "    try:\n",
    "        growth = regex.group(2)\n",
    "    except AttributeError as e:\n",
    "        growth = None\n",
    "\n",
    "    try:\n",
    "        n_sales = regex.group(3)\n",
    "    except AttributeError as e:\n",
    "        n_sales = None\n",
    "\n",
    "    return [median, growth, n_sales]\n",
    "\n",
    "\n",
    "def interactive_regex(data, rows, index):\n",
    "    \"\"\"Parse data from interactive element\"\"\"\n",
    "    try:\n",
    "        h_end = re.search(\n",
    "            r\"high end: \\$?([\\d.?]*k?m?)\", data, re.IGNORECASE).group(1)\n",
    "        low_end = re.search(\n",
    "            r\"entry level: \\$?([\\d.?]*k?m?)\", data, re.IGNORECASE\n",
    "        ).group(1)\n",
    "        rent_median = re.search(\n",
    "            r\"rental median price: \\$?([\\d.?]*k?m?)\", data, re.IGNORECASE\n",
    "        ).group(1)\n",
    "\n",
    "        hist_2022 = re.search(\n",
    "            r\"2022(\\$\\d.*[m|k]{1}|N/A)(-?\\d.*%{1}|-)(\\d*)2021\", data, re.IGNORECASE\n",
    "        )\n",
    "        hist_2022 = yearly_history_parser(hist_2022)\n",
    "\n",
    "        hist_2021 = re.search(\n",
    "            r\"2021(\\$\\d.*[m|k]{1}|N/A)(-?\\d.*%{1}|-)(\\d*)2020\", data, re.IGNORECASE\n",
    "        )\n",
    "        hist_2021 = yearly_history_parser(hist_2021)\n",
    "\n",
    "        hist_2020 = re.search(\n",
    "            r\"2020(\\$\\d.*[m|k]{1}|N/A)(-?\\d.*%{1}|-)(\\d*)2019\", data, re.IGNORECASE\n",
    "        )\n",
    "        hist_2020 = yearly_history_parser(hist_2020)\n",
    "\n",
    "        hist_2019 = re.search(\n",
    "            r\"2019(\\$\\d.*[m|k]{1}|N/A)(-?\\d.*%{1}|-)(\\d*)2018\", data, re.IGNORECASE\n",
    "        )\n",
    "        hist_2019 = yearly_history_parser(hist_2019)\n",
    "\n",
    "        hist_2018 = re.search(\n",
    "            r\"2018(\\$\\d.*[m|k]{1}|N/A)(-?\\d.*%{1}|-)(\\d*)\", data, re.IGNORECASE\n",
    "        )\n",
    "        hist_2018 = yearly_history_parser(hist_2018)\n",
    "\n",
    "        rows[index - 1].extend(\n",
    "            [\n",
    "                h_end,\n",
    "                low_end,\n",
    "                rent_median,\n",
    "                *hist_2022,\n",
    "                *hist_2021,\n",
    "                *hist_2020,\n",
    "                *hist_2019,\n",
    "                *hist_2018,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    except AttributeError as e:\n",
    "        return rows\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def ingester(rows, df, suburb, postcode):\n",
    "    \"\"\"Insert data to dataframe\"\"\"\n",
    "    for i in rows:\n",
    "        if i:\n",
    "            i.insert(0, f\"{i.pop(0)}_{i.pop(0).lower()}\")\n",
    "            del i[5]\n",
    "            i.insert(0, suburb)\n",
    "            i.insert(1, postcode)\n",
    "            # print(i)\n",
    "\n",
    "            df = df.append(\n",
    "                pd.Series(i, index=df.columns[: len(i)]), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def history_scraper(driver, url):\n",
    "    \"\"\"Scrape historical data from Domain\"\"\"\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, features='lxml')\n",
    "    rows = []\n",
    "    try:\n",
    "        table = soup.find(lambda tag: tag.name == \"table\")\n",
    "        n_rows = len(\n",
    "            table.findAll(\n",
    "                lambda tag: tag.name == \"tr\" and tag.findParent(\n",
    "                    \"table\") == table\n",
    "            )\n",
    "        )\n",
    "    except AttributeError as e:\n",
    "        return rows\n",
    "\n",
    "    # Static Rows\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        cols = [td.text for td in tr.find_all(\"td\")]\n",
    "        rows.append(cols)\n",
    "\n",
    "    # Interactive Rows\n",
    "    for i in range(2, n_rows + 1):\n",
    "\n",
    "        driver.find_element(\n",
    "            by=By.CSS_SELECTOR, value=f\".css-168e8ow:nth-child({i}) .css-1vofcfi svg\"\n",
    "        ).click()\n",
    "\n",
    "        soup2 = BeautifulSoup(driver.page_source, features='lxml')\n",
    "\n",
    "        new_data = interactive_data(soup2, i)\n",
    "        rows = interactive_regex(new_data, rows, i)\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f679bea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2868893735.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/tj/r1w0q56s7rv8mhrmmgm8w4cr0000gn/T/ipykernel_21464/2868893735.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    full_suburbs = pd.read_csv(\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "df = df_setup()\n",
    "driver = driver_setup()\n",
    "\n",
    "full_suburbs = pd.read_csv(\n",
    "    \"../data/raw/postcode.csv\", usecols=[\"postcode\", \"locality\", \"state\"]\n",
    ")\n",
    "full_suburbs = full_suburbs[full_suburbs[\"state\"] == \"VIC\"]\n",
    "\n",
    "df = pd.read_csv(\"../data/curated/historical_sales.csv\")\n",
    "for index, row in full_suburbs.iterrows():\n",
    "    if index > 6648:\n",
    "        url, suburb = url_finder(row)\n",
    "        print(url)\n",
    "        try:\n",
    "            table_rows = history_scraper(driver, url)\n",
    "        except ElementClickInterceptedException as e:\n",
    "            continue\n",
    "        df = ingester(table_rows, df, suburb, row[\"postcode\"])\n",
    "        time.sleep(1)\n",
    "        df.to_csv(\"../data/curated/historical_sales.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d67bc8a98929733bfed30279eba52948d85bd0c999fdfd91dc6ab54ef3e87c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
